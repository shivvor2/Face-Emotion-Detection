{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPPajpYCIjTh"
      },
      "source": [
        "# Real time emotion Detection using OpenCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewXK0cu2RTBY"
      },
      "source": [
        "##  Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "conda activate face_emotion_detection\n",
        "\n",
        "-- In windows terminal --\n",
        "\n",
        "usbipd attach --wsl --busid 2-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MQIzBz_JLqj",
        "outputId": "ad5cecac-9158-4856-a80b-c177118ab537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /home/shivvorz/anaconda3/envs/face_emotion_detection/lib/python3.12/site-packages (24.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HHMFZx42PqyT",
        "outputId": "827612b7-638e-46e4-d6ec-7bade934caf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opencv-contrib-python (from -r requirements.txt (line 1))\n",
            "  Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pandas (from -r requirements.txt (line 2))\n",
            "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting huggingface_hub (from -r requirements.txt (line 3))\n",
            "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting numpy (from -r requirements.txt (line 5))\n",
            "  Using cached numpy-2.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting torch (from -r requirements.txt (line 6))\n",
            "  Downloading torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision (from -r requirements.txt (line 7))\n",
            "  Downloading torchvision-0.19.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting python-dotenv (from -r requirements.txt (line 8))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/shivvorz/anaconda3/envs/face_emotion_detection/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0)\n",
            "Collecting pytz>=2020.1 (from pandas->-r requirements.txt (line 2))\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->-r requirements.txt (line 2))\n",
            "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting filelock (from huggingface_hub->-r requirements.txt (line 3))\n",
            "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface_hub->-r requirements.txt (line 3))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/shivvorz/anaconda3/envs/face_emotion_detection/lib/python3.12/site-packages (from huggingface_hub->-r requirements.txt (line 3)) (24.1)\n",
            "Collecting pyyaml>=5.1 (from huggingface_hub->-r requirements.txt (line 3))\n",
            "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting requests (from huggingface_hub->-r requirements.txt (line 3))\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tqdm>=4.42.1 (from huggingface_hub->-r requirements.txt (line 3))\n",
            "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/shivvorz/anaconda3/envs/face_emotion_detection/lib/python3.12/site-packages (from huggingface_hub->-r requirements.txt (line 3)) (4.12.2)\n",
            "Collecting hf-transfer>=0.1.4 (from huggingface_hub[hf_transfer]->-r requirements.txt (line 4))\n",
            "  Downloading hf_transfer-0.1.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting sympy (from torch->-r requirements.txt (line 6))\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch->-r requirements.txt (line 6))\n",
            "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting jinja2 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: setuptools in /home/shivvorz/anaconda3/envs/face_emotion_detection/lib/python3.12/site-packages (from torch->-r requirements.txt (line 6)) (74.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision->-r requirements.txt (line 7))\n",
            "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/shivvorz/anaconda3/envs/face_emotion_detection/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->-r requirements.txt (line 6))\n",
            "  Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->huggingface_hub->-r requirements.txt (line 3))\n",
            "  Downloading charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->huggingface_hub->-r requirements.txt (line 3))\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->huggingface_hub->-r requirements.txt (line 3))\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->huggingface_hub->-r requirements.txt (line 3))\n",
            "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch->-r requirements.txt (line 6))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
            "Using cached numpy-2.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "Downloading torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl (797.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.1-cp312-cp312-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "Downloading hf_transfer-0.1.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
            "Downloading charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytz, mpmath, urllib3, tzdata, tqdm, sympy, pyyaml, python-dotenv, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, hf-transfer, fsspec, filelock, charset-normalizer, certifi, triton, requests, pandas, opencv-contrib-python, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, huggingface_hub, torch, torchvision\n",
            "Successfully installed MarkupSafe-2.1.5 certifi-2024.8.30 charset-normalizer-3.3.2 filelock-3.16.1 fsspec-2024.9.0 hf-transfer-0.1.8 huggingface_hub-0.25.1 idna-3.10 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-2.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 opencv-contrib-python-4.10.0.84 pandas-2.2.3 pillow-10.4.0 python-dotenv-1.0.1 pytz-2024.2 pyyaml-6.0.2 requests-2.32.3 sympy-1.13.3 torch-2.4.1 torchvision-0.19.1 tqdm-4.66.5 triton-3.0.0 tzdata-2024.2 urllib3-2.2.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBSzckGcRbc7",
        "outputId": "02cda606-bd59-47f8-831d-c5b52bf58c45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Accessing webcam via WSL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Follow the instructions (up to section 2) at [this github page](https://github.com/PINTO0309/wsl2_linux_kernel_usbcam_enable_conf?tab=readme-ov-file).\n",
        "\n",
        "Then, perform the following commands in a windows terminal with admin privilages:\n",
        "\n",
        "Install usbipd-win\n",
        "```bash\n",
        "winget install usbipd\n",
        "```\n",
        "\n",
        "Open your list of usb devices\n",
        "```bash\n",
        "usbipd list\n",
        "```\n",
        "\n",
        "Example output:\n",
        "```bash\n",
        "BUSID  VID:PID    DEVICE                                                        STATE\n",
        "2-1    1395:0098  EPOS GSX 300, USB Input Device                                Not shared\n",
        "2-2    046d:085c  C922 Pro Stream Webcam                                        Not shared\n",
        "2-4    0d8c:0050  Thronmax MDrill One Pro, USB Input Device                     Not shared\n",
        "2-5    361d:0100  USB Input Device                                              Not shared\n",
        "```\n",
        "\n",
        "Find the webcam you would want to use and set it to shared\n",
        "```bash\n",
        "usbipd bind --busid=<BUSID>\n",
        "```\n",
        "\n",
        "Then, you may attatch the device to WSL\n",
        "```bash\n",
        "usbipd attach --wsl --busid=<BUSID>\n",
        "```\n",
        "This last step has to be performed every time WSL is launched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trouble Shooting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the following line, the remote agress might be wrong and will cause linux to not be able to retrieve the correct config:\n",
        "```bash\n",
        "$ sudo wget -O .config https://github.com/PINTO0309/wsl2_linux_kernel_usbcam_enable_conf/raw/main/${TAGVER}/config-${WSL_DISTRO_NAME} \\\n",
        "  && sudo chmod 777 .config \\\n",
        "  && sudo make clean\n",
        "```\n",
        "\n",
        "The issue is caused by `WSL_DISTRO_NAME` not including the version number of the ubuntu distro, in that case, input the following command:\n",
        "\n",
        "```bash\n",
        "lsb_release -a\n",
        "```\n",
        "\n",
        "Example output:\n",
        "```bash\n",
        "No LSB modules are available.\n",
        "Distributor ID: Ubuntu\n",
        "Description:    Ubuntu 22.04.5 LTS\n",
        "Release:        22.04\n",
        "Codename:       jammy\n",
        "```\n",
        "\n",
        "Then simply run the original line again but replacing `${WSL_DISTRO_NAME}` with `Ubuntu-<Release>` where `<Release>` is the \"Release\" row of the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWwTIfRGR6yn"
      },
      "source": [
        "## Downloading models from OpenCV model Zoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmFK5uTzdUvz",
        "outputId": "5ef237b8-6b96-437c-b7ce-af1039347fde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'opencv_zoo'...\n",
            "remote: Enumerating objects: 1933, done.\u001b[K\n",
            "remote: Counting objects: 100% (891/891), done.\u001b[K\n",
            "remote: Compressing objects: 100% (390/390), done.\u001b[K\n",
            "remote: Total 1933 (delta 664), reused 584 (delta 499), pack-reused 1042 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1933/1933), 1.21 MiB | 1.18 MiB/s, done.\n",
            "Resolving deltas: 100% (1207/1207), done.\n",
            "Updating files: 100% (277/277), done.\n",
            "git: 'lfs' is not a git command. See 'git --help'.\n",
            "\n",
            "The most similar command is\n",
            "\tlog\n",
            "git: 'lfs' is not a git command. See 'git --help'.\n",
            "\n",
            "The most similar command is\n",
            "\tlog\n"
          ]
        }
      ],
      "source": [
        "#Cloning the opencv zoo repo\n",
        "!git clone https://github.com/opencv/opencv_zoo\n",
        "\n",
        "# installing git-lfs\n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs install\n",
        "\n",
        "# Loading large file from opencv_zoo\n",
        "!cd opencv_zoo\n",
        "# # If only want to pull required models\n",
        "# !git config lfs.fetchinclude \"models/face_detection_yunet, models/facial_expression_recognition\"\n",
        "!git lfs pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "friNPDxBV_v-"
      },
      "source": [
        "## Face and emotion reicognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from opencv_zoo.models.face_detection_yunet.yunet import YuNet\n",
        "from opencv_zoo.models.facial_expression_recognition.facial_fer_model import FacialExpressionRecog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale the camera input\n",
        "scaling_ratio = 1.0 \n",
        "\n",
        "# Add \"_int8\" to run a quantized version of the model e.g. \"...yunet_2023mar_int8\"\n",
        "face_detector_model_path = \"opencv_zoo/models/face_detection_yunet/face_detection_yunet_2023mar.onnx\" \n",
        "experssion_detector_model_path = \"opencv_zoo/models/facial_expression_recognition/facial_expression_recognition_mobilefacenet_2022july.onnx\"\n",
        "\n",
        "# Verbosity\n",
        "verbose = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get emotions from recognized faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_faces(frame, faces, model: FacialExpressionRecog, verbose = True):\n",
        "    faces_emotion = []\n",
        "    # Skip processing if no faces detected\n",
        "    # if not faces:\n",
        "    #     return faces_emotion\n",
        "    \n",
        "    for face in faces:\n",
        "        face_coords = face[:-1].astype(np.int32)\n",
        "        infernece_result = model.infer(frame, face_coords)[0] # model.infer outputs a single element list\n",
        "        emotion = model.getDesc(infernece_result)\n",
        "        faces_emotion.append(emotion)\n",
        "    \n",
        "    if verbose:\n",
        "        faces_emotion_results = [\"Face {}: {}\".format(idx, emotion) for idx, emotion in enumerate(faces_emotion)]\n",
        "        print(faces_emotion_results)\n",
        "    \n",
        "    return faces_emotion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization of Model outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "z-v0NbyVcn9r"
      },
      "outputs": [],
      "source": [
        "def visualize(input, faces, emotions, fps, thickness=2, show_facial_feature_points = False, verbose = True):\n",
        "    if faces is not None:\n",
        "        for idx, face in enumerate(faces):\n",
        "            print('Face {}, top-left coordinates: ({:.0f}, {:.0f}), box width: {:.0f}, box height {:.0f}, score: {:.2f}'.format(idx, face[0], face[1], face[2], face[3], face[-1]))\n",
        "\n",
        "            coords = face[:-1].astype(np.int32)\n",
        "            cv.rectangle(input, (coords[0], coords[1]), (coords[0]+coords[2], coords[1]+coords[3]), (0, 200, 200), thickness)\n",
        "            cv.putText(input, emotions[idx] ,(coords[0], coords[1]), cv.FONT_HERSHEY_PLAIN, 1.5, (255,0,0))\n",
        "            if show_facial_feature_points:\n",
        "                cv.circle(input, (coords[4], coords[5]), 1, (128, 0, 128), thickness)\n",
        "                cv.circle(input, (coords[6], coords[7]), 1, (128, 0, 128), thickness)\n",
        "                cv.circle(input, (coords[8], coords[9]), 1, (128, 0, 128), thickness)\n",
        "                cv.circle(input, (coords[10], coords[11]), 1, (128, 0, 128), thickness)\n",
        "                cv.circle(input, (coords[12], coords[13]), 1, (128, 0, 128), thickness)\n",
        "    cv.putText(input, 'FPS: {:.2f}'.format(fps), (1, 16), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialization of model instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BNWevKc2V_W5"
      },
      "outputs": [],
      "source": [
        "# Initializing models\n",
        "face_detector = YuNet(modelPath = face_detector_model_path,\n",
        "                      inputSize = [512,512],\n",
        "                      confThreshold = 0.75,\n",
        "                      nmsThreshold = 0.3)\n",
        "expression_detector = FacialExpressionRecog(modelPath=experssion_detector_model_path)\n",
        "\n",
        "tm = cv.TickMeter()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0A3WWaNYnT1",
        "outputId": "38167087-6b03-43cd-8305-1045b060f12b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (232, 173), box width: 155, box height 219, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (232, 173), box width: 154, box height 216, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (232, 171), box width: 154, box height 218, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (233, 174), box width: 152, box height 218, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (234, 174), box width: 153, box height 218, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (234, 174), box width: 153, box height 217, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (237, 176), box width: 152, box height 212, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (238, 178), box width: 154, box height 207, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (238, 181), box width: 154, box height 205, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (237, 180), box width: 153, box height 204, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (235, 178), box width: 154, box height 206, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (234, 178), box width: 154, box height 204, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (233, 179), box width: 155, box height 203, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (233, 178), box width: 155, box height 206, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (233, 175), box width: 154, box height 224, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (232, 173), box width: 156, box height 213, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (228, 175), box width: 158, box height 203, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (228, 178), box width: 157, box height 199, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (227, 176), box width: 159, box height 202, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (227, 176), box width: 159, box height 206, score: 0.94\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (232, 172), box width: 151, box height 219, score: 0.94\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (229, 169), box width: 153, box height 214, score: 0.95\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (225, 174), box width: 156, box height 209, score: 0.95\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (225, 175), box width: 155, box height 206, score: 0.95\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (225, 174), box width: 155, box height 208, score: 0.95\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (225, 174), box width: 156, box height 208, score: 0.95\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (224, 174), box width: 156, box height 207, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (227, 176), box width: 156, box height 204, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (227, 176), box width: 155, box height 203, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (226, 175), box width: 156, box height 205, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (231, 169), box width: 150, box height 216, score: 0.94\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (226, 176), box width: 156, box height 202, score: 0.95\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (225, 175), box width: 156, box height 209, score: 0.94\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (232, 182), box width: 153, box height 202, score: 0.94\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (231, 181), box width: 155, box height 204, score: 0.94\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (226, 182), box width: 158, box height 203, score: 0.94\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (224, 185), box width: 159, box height 202, score: 0.94\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (225, 183), box width: 158, box height 204, score: 0.94\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (225, 184), box width: 159, box height 200, score: 0.94\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (225, 183), box width: 157, box height 204, score: 0.94\n",
            "['Face 0: disgust']\n",
            "Face 0, top-left coordinates: (226, 178), box width: 157, box height 205, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (233, 174), box width: 151, box height 211, score: 0.95\n",
            "['Face 0: disgust']\n",
            "Face 0, top-left coordinates: (239, 170), box width: 152, box height 206, score: 0.94\n",
            "['Face 0: disgust']\n",
            "Face 0, top-left coordinates: (238, 169), box width: 154, box height 207, score: 0.94\n",
            "['Face 0: sad']\n",
            "Face 0, top-left coordinates: (237, 171), box width: 154, box height 206, score: 0.94\n",
            "['Face 0: sad']\n",
            "Face 0, top-left coordinates: (237, 171), box width: 154, box height 201, score: 0.94\n",
            "['Face 0: sad']\n",
            "Face 0, top-left coordinates: (237, 171), box width: 153, box height 203, score: 0.94\n",
            "['Face 0: sad']\n",
            "Face 0, top-left coordinates: (235, 170), box width: 151, box height 202, score: 0.94\n",
            "['Face 0: sad']\n",
            "Face 0, top-left coordinates: (235, 170), box width: 151, box height 202, score: 0.94\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (234, 171), box width: 150, box height 203, score: 0.95\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (232, 170), box width: 149, box height 207, score: 0.95\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (223, 169), box width: 158, box height 201, score: 0.95\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (208, 165), box width: 158, box height 208, score: 0.94\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (201, 175), box width: 152, box height 201, score: 0.95\n",
            "['Face 0: happy']\n",
            "Face 0, top-left coordinates: (215, 166), box width: 158, box height 205, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (232, 162), box width: 160, box height 204, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (241, 170), box width: 157, box height 206, score: 0.93\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (242, 172), box width: 157, box height 209, score: 0.93\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (239, 170), box width: 157, box height 205, score: 0.93\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (237, 170), box width: 156, box height 202, score: 0.93\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (237, 167), box width: 156, box height 204, score: 0.93\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (237, 168), box width: 157, box height 201, score: 0.93\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (237, 167), box width: 155, box height 202, score: 0.93\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (237, 165), box width: 155, box height 204, score: 0.93\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (237, 165), box width: 154, box height 202, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (233, 165), box width: 152, box height 208, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (227, 166), box width: 152, box height 206, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (224, 163), box width: 154, box height 206, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (227, 171), box width: 151, box height 199, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (225, 172), box width: 149, box height 198, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (225, 171), box width: 149, box height 199, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (225, 169), box width: 147, box height 206, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (224, 162), box width: 152, box height 208, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (225, 163), box width: 153, box height 208, score: 0.95\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (227, 165), box width: 152, box height 213, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (238, 189), box width: 153, box height 214, score: 0.94\n",
            "['Face 0: neutral']\n",
            "Face 0, top-left coordinates: (239, 195), box width: 162, box height 214, score: 0.94\n"
          ]
        }
      ],
      "source": [
        "deviceId = 0\n",
        "cap = cv.VideoCapture(deviceId)\n",
        "frameWidth = int(cap.get(cv.CAP_PROP_FRAME_WIDTH) * scaling_ratio)\n",
        "frameHeight = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT) * scaling_ratio)\n",
        "face_detector.setInputSize([frameWidth, frameHeight])\n",
        "\n",
        "while True:\n",
        "    # Break if cannot capture image\n",
        "    hasFrame, frame = cap.read()\n",
        "    if not hasFrame:\n",
        "        print('No input detected ...')\n",
        "        break\n",
        "    \n",
        "    pressedKey = cv.waitKey(1) & 0xFF\n",
        "    if pressedKey == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "    frame = cv.resize(frame, (frameWidth, frameHeight))\n",
        "\n",
        "    # Inference\n",
        "    tm.start()\n",
        "    faces = face_detector.infer(frame) # faces is a tuple\n",
        "    emotions = process_faces(frame, faces, expression_detector, verbose)\n",
        "    tm.stop()\n",
        "\n",
        "    # Draw results on the input image\n",
        "    visualize(frame, faces, emotions, tm.getFPS(), verbose = verbose)\n",
        "\n",
        "    # Visualize results\n",
        "    cv.imshow('Live', frame)\n",
        "    \n",
        "cap.release()\n",
        "cv.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
